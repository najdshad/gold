{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3868/1300698555.py:5: FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
      "  df = pd.read_csv(\n",
      "/tmp/ipykernel_3868/1300698555.py:5: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete.\n"
     ]
    }
   ],
   "source": [
    "def transform_and_calculate(file_path, stop_distance=6, rr_ratio=3, lookahead=30):\n",
    "    # 1. Data transformation\n",
    "    def load_and_transform(file_path):\n",
    "        # Load CSV with proper datetime handling\n",
    "        df = pd.read_csv(\n",
    "            file_path,\n",
    "            delimiter='\\t',\n",
    "            parse_dates={'datetime': ['<DATE>', '<TIME>']},\n",
    "            date_parser=lambda x: pd.to_datetime(x, format='%Y.%m.%d %H:%M:%S'),\n",
    "            usecols=['<DATE>', '<TIME>', '<OPEN>', '<HIGH>', '<LOW>', '<CLOSE>', '<TICKVOL>', '<SPREAD>']\n",
    "        )\n",
    "        \n",
    "        # Clean column names\n",
    "        df.columns = df.columns.str.strip('<>').str.lower()\n",
    "        \n",
    "        # Column renaming and reorganization\n",
    "        df = df.rename(columns={'tickvol': 'vol'})\n",
    "        df = df[['datetime', 'open', 'high', 'low', 'close', 'vol', 'spread']]\n",
    "        \n",
    "        return df.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "    # 2. Spread-adjusted condition calculation\n",
    "    def calculate_condition_met(df, stop_distance, rr_ratio, lookahead):\n",
    "        # Initialize both condition columns\n",
    "        df['long_condition'] = False\n",
    "        df['short_condition'] = False\n",
    "        \n",
    "        for i in range(len(df)):\n",
    "            current = df.iloc[i]\n",
    "            future = df.iloc[i+1:i+lookahead+1]\n",
    "            \n",
    "            if future.empty:\n",
    "                break\n",
    "            \n",
    "            # Calculate entry prices with spread adjustment\n",
    "            long_entry = current['close'] + current['spread'] * 0.01\n",
    "            short_entry = current['close'] - current['spread'] * 0.01\n",
    "            \n",
    "            # Calculate price levels with spread\n",
    "            long_stop = long_entry - stop_distance\n",
    "            long_target = long_entry + (stop_distance * rr_ratio)\n",
    "            short_stop = short_entry + stop_distance\n",
    "            short_target = short_entry - (stop_distance * rr_ratio)\n",
    "            \n",
    "            # Track both scenarios\n",
    "            long_status = {'met': False, 'stopped': False}\n",
    "            short_status = {'met': False, 'stopped': False}\n",
    "            \n",
    "            for _, future_candle in future.iterrows():\n",
    "            # Check long condition if not yet resolved\n",
    "                if not long_status['met'] and not long_status['stopped']:\n",
    "                    # Check if price hit stop loss first\n",
    "                    if future_candle['low'] <= long_stop:\n",
    "                        long_status['stopped'] = True\n",
    "                    # Check if price hit take profit first\n",
    "                    elif future_candle['high'] >= long_target:\n",
    "                        long_status['met'] = True\n",
    "\n",
    "                # Check short condition if not yet resolved\n",
    "                if not short_status['met'] and not short_status['stopped']:\n",
    "                    # Check if price hit stop loss first\n",
    "                    if future_candle['high'] >= short_stop:\n",
    "                        short_status['stopped'] = True\n",
    "                    # Check if price hit take profit first\n",
    "                    elif future_candle['low'] <= short_target:\n",
    "                        short_status['met'] = True\n",
    "\n",
    "                # Early exit if both directions are resolved\n",
    "                # if (long_status['met'] or long_status['stopped']) and \\\n",
    "                #     (short_status['met'] or short_status['stopped']):\n",
    "                #     break\n",
    "\n",
    "            # Record results in DataFrame\n",
    "            df.at[i, 'long_condition'] = long_status['met']\n",
    "            df.at[i, 'short_condition'] = short_status['met']\n",
    "        \n",
    "        return df\n",
    "\n",
    "    # Execute processing pipeline\n",
    "    df = load_and_transform(file_path)\n",
    "    df = calculate_condition_met(df, stop_distance, rr_ratio, lookahead)\n",
    "    return df\n",
    "\n",
    "# Usage example:\n",
    "csv_path = 'Data/MT5/XAUUSD_M15_202012070900_202502282345.csv'\n",
    "processed_data = transform_and_calculate(csv_path)\n",
    "processed_data.to_csv('Processed_Data/15m_3r.csv', index=False)\n",
    "print(\"Data processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 60, 9])\n",
      "Target shape: torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "class CandlestickDataset(Dataset):\n",
    "    def __init__(self, csv_path, lookback=60, feature_columns=['open', 'high', 'low', 'close']):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (str): Path to OHLC csv file with 'long_condition' and 'short_condition' columns\n",
    "            lookback (int): Historical window size in timesteps\n",
    "            feature_columns (list): Columns to use as features\n",
    "        \"\"\"\n",
    "        # Load and preprocess data\n",
    "        self.data = pd.read_csv(csv_path, parse_dates=['datetime'])\n",
    "        self.data.sort_values('datetime', inplace=True)\n",
    "        self.features = self.data[feature_columns].values\n",
    "        self.dates = self.data['datetime'].values\n",
    "        self.targets = self.data[['long_condition', 'short_condition']].values.astype(np.float32)\n",
    "        \n",
    "        # Normalize prices using rolling Z-score\n",
    "        self._normalize_data()\n",
    "        \n",
    "        self.lookback = lookback\n",
    "\n",
    "    def _normalize_data(self):\n",
    "        \"\"\"Applies rolling Z-score normalization\"\"\"\n",
    "        rolling_mean = self.data['close'].rolling(window=100, min_periods=1).mean()\n",
    "        rolling_std = self.data['close'].rolling(window=100, min_periods=1).std()\n",
    "        \n",
    "        for col in ['open', 'high', 'low', 'close']:\n",
    "            self.data[f'{col}_norm'] = (self.data[col] - rolling_mean) / rolling_std\n",
    "        self.features = self.data[[f'{c}_norm' for c in ['open', 'high', 'low', 'close']]].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns:\n",
    "            x: (lookback, 4 + temporal_features) tensor of features\n",
    "            y: (2,) tensor of [long_profit, short_profit] at prediction point\n",
    "        \"\"\"\n",
    "        # Get sequence window\n",
    "        sequence = self.features[idx:idx + self.lookback]\n",
    "        sequence_dates = self.dates[idx:idx + self.lookback]\n",
    "        \n",
    "        # Get target for the last candle in the sequence\n",
    "        target_idx = idx + self.lookback - 1\n",
    "        target = self.targets[target_idx]\n",
    "        \n",
    "        # Add temporal encoding\n",
    "        sequence = self._add_temporal_features(sequence, sequence_dates)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        x = torch.FloatTensor(sequence)\n",
    "        y = torch.FloatTensor(target)\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "    def _add_temporal_features(self, hist, dates):\n",
    "        \"\"\"Adds time-related features to the sequence\"\"\"\n",
    "        dt_features = []\n",
    "        for ts in dates:\n",
    "            dt = pd.to_datetime(ts)\n",
    "            dt_features.append([\n",
    "                dt.dayofweek / 7,\n",
    "                dt.hour / 24,\n",
    "                dt.month / 12,\n",
    "                dt.is_month_end,\n",
    "                dt.is_quarter_end,\n",
    "            ])\n",
    "        \n",
    "        return np.concatenate([hist, np.array(dt_features)], axis=1)\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = CandlestickDataset('Processed_Data/15m_3r.csv', lookback=60)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Test batch\n",
    "    x, y = next(iter(dataloader))\n",
    "    print(f\"Input shape: {x.shape}\")  # (32, 60, 9)\n",
    "    print(f\"Target shape: {y.shape}\")  # (32, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Implements the sine and cosine positional encoding.\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # [max_len, d_model]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # [max_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # apply sine on even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # apply cosine on odd indices\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape [batch_size, seq_len, d_model]\n",
    "        Returns:\n",
    "            x with positional encodings added.\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len]\n",
    "\n",
    "class PriceTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer model template specialized for processing price data sorted by datetime.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, d_model, nhead, num_layers, dim_feedforward, dropout=0.1, max_seq_length=5000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Number of features in the raw price data.\n",
    "            d_model: Dimension of the model (embedding dimension).\n",
    "            nhead: Number of attention heads.\n",
    "            num_layers: Number of Transformer encoder layers.\n",
    "            dim_feedforward: Dimension of the feedforward network inside each encoder layer.\n",
    "            dropout: Dropout probability.\n",
    "            max_seq_length: Maximum sequence length for positional encoding.\n",
    "        \"\"\"\n",
    "        super(PriceTransformer, self).__init__()\n",
    "        # Project input features into d_model dimensions.\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len=max_seq_length)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, \n",
    "                                                   dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Final regression head (e.g., to predict a future price value).\n",
    "        self.fc_out = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor of shape [batch_size, seq_len, input_dim] containing price data sorted by datetime.\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, seq_len, 1] representing model predictions.\n",
    "        \"\"\"\n",
    "        x = self.input_proj(src)            # [batch, seq_len, d_model]\n",
    "        x = self.pos_encoder(x)             # add positional encoding\n",
    "        x = x.transpose(0, 1)               # Transformer expects [seq_len, batch, d_model]\n",
    "        x = self.transformer_encoder(x)     # apply encoder layers\n",
    "        x = x.transpose(0, 1)               # back to [batch, seq_len, d_model]\n",
    "        x = self.dropout(x)\n",
    "        output = self.fc_out(x)             # regression output for each timestep\n",
    "        return output\n",
    "\n",
    "# -----------------------\n",
    "# Training Loop Section\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Hyperparameters\n",
    "    batch_size = 32\n",
    "    seq_len = 250\n",
    "    input_dim = 4          # e.g., [open, high, low, close] price features\n",
    "    num_epochs = 100\n",
    "    learning_rate = 0.003\n",
    "\n",
    "    # Create dummy dataset\n",
    "    dummy_input = torch.randn(batch_size, seq_len, input_dim)\n",
    "    # Create dummy target values (regression targets)\n",
    "    dummy_target = torch.randn(batch_size, seq_len, 1)\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = PriceTransformer(input_dim=input_dim, d_model=64, nhead=8, \n",
    "                             num_layers=3, dim_feedforward=128, dropout=0.1, max_seq_length=200)\n",
    "    model.train()  # set the model to training mode\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()  # Mean Squared Error loss for regression\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()          # clear previous gradients\n",
    "        predictions = model(dummy_input)  # forward pass\n",
    "        loss = criterion(predictions, dummy_target)  # compute loss\n",
    "        loss.backward()                # backpropagate gradients\n",
    "        optimizer.step()               # update model parameters\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
